# User Feedback & Training Guide

**System**: Email Intelligence Workflow System
**Version**: 1.0 (MVP)
**Last Updated**: 2025-11-22

---

## Overview

The Email Intelligence Workflow System learns from your corrections using a **database-driven feedback mechanism**. You provide feedback by directly editing classifications in Supabase, and the system automatically logs corrections and uses them to improve future classifications.

---

## How to Provide Feedback (Correct Misclassifications)

### Step 1: Access Supabase Dashboard

1. Navigate to: https://supabase.com/dashboard/project/xmziovusqlmgygcrgyqt
2. Log in with your Supabase credentials
3. Click on **"Table Editor"** in the left sidebar

### Step 2: Review Classifications

1. Select the **`classifications`** table from the dropdown
2. Browse recent classifications (sorted by `classified_timestamp` DESC by default)
3. Look for entries with:
   - Low `confidence_score` (< 0.7) - these are more likely to be incorrect
   - Unexpected `category`, `urgency`, or `action` values

**Useful Filters**:
```sql
-- Find low-confidence classifications
WHERE confidence_score < 0.7

-- Find recent classifications (last 24 hours)
WHERE classified_timestamp > NOW() - INTERVAL '24 hours'

-- Find specific category
WHERE category = 'OTHER'  -- often a catch-all that needs correction
```

### Step 3: Join with Email Data

To see the actual email content alongside classifications:

1. Click **"SQL Editor"** in the left sidebar
2. Run this query:

```sql
SELECT
  e.subject,
  e.sender,
  e.received_at,
  c.category,
  c.urgency,
  c.action,
  c.confidence_score,
  c.id as classification_id
FROM emails e
JOIN classifications c ON e.id = c.email_id
WHERE c.classified_timestamp > NOW() - INTERVAL '7 days'
ORDER BY c.classified_timestamp DESC
LIMIT 50;
```

3. Review email context to determine if classification is correct

### Step 4: Correct Misclassifications

1. Return to **Table Editor** ‚Üí **`classifications`** table
2. Click the row you want to correct (this opens the edit panel)
3. Modify the incorrect field(s):
   - **`category`**: Change to correct category (KIDS, ROBYN, WORK, FINANCIAL, SHOPPING, OTHER)
   - **`urgency`**: Change to correct urgency (HIGH, MEDIUM, LOW)
   - **`action`**: Change to correct action (FYI, RESPOND, TASK, PAYMENT, CALENDAR, NONE)
4. Click **"Save"** button

**Important**:
- ‚úÖ **DO** edit the main fields: `category`, `urgency`, `action`
- ‚ùå **DON'T** edit `original_*` fields (these are auto-populated by the trigger)
- ‚ùå **DON'T** edit `corrected_timestamp` or `corrected_by` (auto-populated)

### Step 5: Verify Correction Was Logged

1. Navigate to **Table Editor** ‚Üí **`correction_logs`** table
2. You should see new entries for your correction:
   - One row per changed field (category, urgency, action)
   - `original_value`: The AI's classification
   - `corrected_value`: Your correction
   - `correction_timestamp`: When you made the correction
   - `corrected_by`: Your database user identifier

**Example correction log entry**:
```
email_id: 123
field_name: CATEGORY
original_value: SHOPPING
corrected_value: WORK
correction_timestamp: 2025-11-22 10:30:00
corrected_by: authenticated_user
```

---

## How the System Learns from Your Corrections

### Immediate Learning (Real-Time)

**Vector Similarity Search** uses your corrections automatically:

1. **Before correction**: AI finds similar emails and sees incorrect classifications
2. **After correction**: AI finds similar emails and sees **your corrected** classifications
3. **Future emails**: AI uses corrected classifications as context for new emails

**Example**:
- Email from `orders@workvendor.com` was classified as SHOPPING (incorrect)
- You correct it to WORK
- Next email from `orders@workvendor.com` ‚Üí AI finds your previous correction via similarity search
- AI is more likely to classify new email as WORK (learned from your correction)

**No action required** - this happens automatically!

### Long-Term Learning (Periodic)

**Correction logs** build a dataset for systematic improvements:

#### Monthly Review Process

Run these queries to identify patterns:

**1. Most Frequently Corrected Categories**
```sql
SELECT
  original_value as ai_classification,
  corrected_value as correct_classification,
  COUNT(*) as correction_count
FROM correction_logs
WHERE field_name = 'CATEGORY'
GROUP BY original_value, corrected_value
ORDER BY correction_count DESC
LIMIT 10;
```

**Example output**:
```
ai_classification | correct_classification | correction_count
SHOPPING         | WORK                   | 15
OTHER            | FINANCIAL              | 8
WORK             | KIDS                   | 3
```

**Action**: If SHOPPING‚ÜíWORK corrections are frequent:
- Investigate emails: Are they from a work vendor?
- Update AI prompt: Add few-shot example for this pattern
- Example: "Emails from vendor@company.com with 'Order Confirmation' should be classified as WORK, not SHOPPING"

**2. Correction Rate by Confidence Score**
```sql
SELECT
  FLOOR(c.confidence_score * 10) / 10 as confidence_bucket,
  COUNT(*) as total_classifications,
  COUNT(c.original_category) as corrections,
  ROUND(100.0 * COUNT(c.original_category) / COUNT(*), 2) as correction_rate_pct
FROM classifications c
WHERE c.classified_timestamp > NOW() - INTERVAL '30 days'
GROUP BY confidence_bucket
ORDER BY confidence_bucket DESC;
```

**Example output**:
```
confidence_bucket | total_classifications | corrections | correction_rate_pct
0.9               | 120                  | 2           | 1.67%
0.8               | 45                   | 5           | 11.11%
0.7               | 20                   | 8           | 40.00%
0.6               | 10                   | 7           | 70.00%
```

**Action**: If correction rate is high for confidence 0.6-0.7:
- Increase confidence threshold to 0.75 (flag more emails for manual review)
- Add more specific examples to AI prompt

**3. Sender-Specific Corrections**
```sql
SELECT
  e.sender,
  cl.original_value as ai_classification,
  cl.corrected_value as correct_classification,
  COUNT(*) as correction_count
FROM correction_logs cl
JOIN emails e ON cl.email_id = e.id
WHERE cl.field_name = 'CATEGORY'
GROUP BY e.sender, cl.original_value, cl.corrected_value
HAVING COUNT(*) >= 2
ORDER BY correction_count DESC;
```

**Example output**:
```
sender                  | ai_classification | correct_classification | correction_count
schoolstore@school.com  | SHOPPING         | KIDS                   | 5
robyn@personal.com      | OTHER            | ROBYN                  | 4
```

**Action**: Add sender-specific rules to AI prompt:
- "Emails from schoolstore@school.com should be classified as KIDS (school fundraisers)"
- "Emails from robyn@personal.com should always be classified as ROBYN"

---

## Improving the AI Prompt Based on Corrections

### Step-by-Step Process

#### 1. Identify Pattern from Correction Logs

Run the queries above to find frequent misclassification patterns.

**Example finding**:
- 15 emails from work vendors classified as SHOPPING but should be WORK
- Pattern: Subject contains "Order Confirmation" and sender is a business vendor

#### 2. Update AI Prompt in Classification-Workflow

1. Open n8n: https://n8n.coxserver.com
2. Navigate to **Classification-Workflow** (ID: MVkAVroogGQA6ePC)
3. Click on **"AI Classification Agent"** node
4. Find the **System Message** (classification prompt)
5. Add a **few-shot example** for the pattern:

**Before**:
```
CATEGORIES:
- WORK: Professional emails, meetings, projects
- SHOPPING: Order confirmations, shipping, promotions
```

**After** (add specific example):
```
CATEGORIES:
- WORK: Professional emails, meetings, projects, business vendor orders
  Example: Email from vendor@supplier.com with subject "Order Confirmation #12345"
  should be WORK (business purchase), not SHOPPING (personal purchase)

- SHOPPING: Personal order confirmations, personal shipping, personal promotions
  Example: Email from orders@amazon.com should be SHOPPING
```

6. Click **"Save"** in n8n
7. Test with a few new emails to verify improvement

#### 3. Monitor Correction Rate

After updating the prompt, monitor correction rate for 1-2 weeks:

```sql
SELECT
  DATE_TRUNC('week', correction_timestamp) as week,
  COUNT(*) as corrections
FROM correction_logs
WHERE field_name = 'CATEGORY'
GROUP BY week
ORDER BY week DESC
LIMIT 4;
```

**Example output**:
```
week       | corrections
2025-11-18 | 3   (after prompt update)
2025-11-11 | 12  (before prompt update)
2025-11-04 | 15
2025-10-28 | 18
```

**Success**: Correction rate decreased from 12-15/week to 3/week ‚úÖ

---

## Advanced: Retraining with Corrections (Phase 3+)

### Fine-Tuning OpenAI Model (Future)

Once you have 100+ corrections, you can fine-tune the GPT model:

**1. Export Training Data**
```sql
COPY (
  SELECT
    e.subject || ' | From: ' || e.sender || ' | ' || LEFT(e.body, 500) as prompt,
    c.category || ',' || c.urgency || ',' || c.action as completion
  FROM corrections_logs cl
  JOIN emails e ON cl.email_id = e.id
  JOIN classifications c ON cl.email_id = c.email_id
  WHERE cl.field_name = 'CATEGORY'
) TO '/tmp/training_data.csv' WITH CSV HEADER;
```

**2. Format for OpenAI Fine-Tuning**
```json
{
  "messages": [
    {"role": "system", "content": "You are an email classifier..."},
    {"role": "user", "content": "Subject: Order Confirmation | From: vendor@work.com | ..."},
    {"role": "assistant", "content": "WORK,MEDIUM,FYI"}
  ]
}
```

**3. Fine-Tune Model**
```bash
# Upload training file
openai api files.create -f training_data.jsonl -p fine-tune

# Create fine-tuning job
openai api fine_tuning.jobs.create \
  -t file-abc123 \
  -m gpt-4.1-mini \
  --suffix "email-classifier-v2"

# Wait for completion (2-4 hours)
# Result: ft:gpt-4.1-mini:org:email-classifier-v2:abc123
```

**4. Update n8n to Use Fine-Tuned Model**
- Open Classification-Workflow ‚Üí OpenAI Chat Model node
- Change model from `gpt-4.1-mini` to `ft:gpt-4.1-mini:org:email-classifier-v2:abc123`
- Save and test

### Updating Vector Embeddings (Future)

If using custom embeddings (not OpenAI):

**1. Retrain Embeddings on Corrected Data**
```python
# Fetch corrected classifications
corrected_emails = fetch_corrected_classifications()

# Retrain embedding model
model.train(corrected_emails)

# Generate new embeddings
new_embeddings = model.embed(corrected_emails)

# Update email_embeddings table
update_embeddings(new_embeddings)
```

**2. Rebuild Vector Index**
```sql
-- Drop old index
DROP INDEX idx_email_embeddings_vector;

-- Rebuild with new embeddings
CREATE INDEX idx_email_embeddings_vector ON email_embeddings
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

---

## Best Practices for Providing Feedback

### 1. Review Regularly

**Recommended schedule**:
- **Daily**: Review high-priority emails (urgency=HIGH, action=PAYMENT/CALENDAR) - ensure no misclassifications
- **Weekly**: Review 10-20 recent classifications for accuracy
- **Monthly**: Run correction analytics queries to identify patterns

### 2. Focus on High-Impact Corrections

**Prioritize correcting**:
- ‚úÖ **High-confidence errors** (confidence > 0.8 but still wrong) - these improve AI prompt
- ‚úÖ **High-urgency errors** (classified as LOW but should be HIGH) - these prevent missed deadlines
- ‚úÖ **Repeated patterns** (same sender/subject pattern misclassified) - these have compound impact
- ‚ùå **Skip low-confidence, low-impact** (confidence < 0.5, urgency=LOW) - these take time with minimal benefit

### 3. Add Context Notes

When correcting, you can add notes to `correction_reason` field:

```sql
-- Update with reason
UPDATE classifications
SET
  category = 'WORK',
  correction_reason = 'Email from work vendor, not personal shopping'
WHERE id = 123;
```

This helps when reviewing correction logs later.

### 4. Test After Prompt Updates

After updating the AI prompt based on corrections:

1. **Send test emails** that match the corrected pattern
2. **Verify classification** is now correct
3. **Monitor for 1-2 weeks** to ensure improvement holds

### 5. Track Overall Accuracy

**Monthly accuracy check**:
```sql
-- Overall correction rate (lower is better)
SELECT
  COUNT(*) as total_classifications,
  COUNT(original_category) as corrections,
  ROUND(100.0 * COUNT(original_category) / COUNT(*), 2) as correction_rate_pct,
  100.0 - ROUND(100.0 * COUNT(original_category) / COUNT(*), 2) as accuracy_pct
FROM classifications
WHERE classified_timestamp > NOW() - INTERVAL '30 days';
```

**Target**: 95%+ accuracy (< 5% correction rate)

**Current status** (from production data):
- Accuracy: 100% (4/4 test emails correct)
- Correction rate: 0% (no corrections needed yet)

---

## Troubleshooting

### Issue: Correction Not Logged

**Symptoms**:
- You edited classification in Supabase
- No entry appears in `correction_logs` table

**Diagnosis**:
```sql
-- Check if trigger exists
SELECT * FROM pg_trigger
WHERE tgname = 'log_corrections_before_update';

-- Check if function exists
SELECT proname FROM pg_proc
WHERE proname = 'log_classification_correction';
```

**Solution**:
1. Verify trigger is active (should exist on `classifications` table)
2. Re-run trigger creation SQL from `data-model.md`
3. Test with a manual correction

### Issue: Original Values Not Preserved

**Symptoms**:
- `original_category` is NULL after correction
- Cannot compare AI vs corrected values

**Diagnosis**:
```sql
-- Check if original_* fields exist
SELECT column_name
FROM information_schema.columns
WHERE table_name = 'classifications'
AND column_name LIKE 'original_%';
```

**Solution**:
1. Ensure trigger is firing BEFORE UPDATE (not AFTER)
2. Check trigger function preserves originals only on first correction
3. Re-run `log_classification_correction` function from schema

### Issue: Vector Search Not Using Corrections

**Symptoms**:
- AI still makes same mistakes after corrections
- Similar emails show old (incorrect) classifications

**Diagnosis**:
```sql
-- Check if Similar Emails Tool finds corrected values
SELECT
  email_id,
  category,
  original_category,
  corrected_timestamp
FROM classifications
WHERE corrected_timestamp IS NOT NULL
ORDER BY corrected_timestamp DESC;
```

**Solution**:
- Corrections are automatically used (no action needed)
- Vector search queries `classifications` table with current values
- If AI still makes mistakes, the emails may not be similar enough (similarity < 0.7)
- Consider lowering similarity threshold in Vector Store node from 0.7 to 0.6

---

## Metrics to Track

### Key Performance Indicators

**1. Correction Rate** (Target: < 5%)
```sql
SELECT
  ROUND(100.0 * COUNT(original_category) / COUNT(*), 2) as correction_rate_pct
FROM classifications
WHERE classified_timestamp > NOW() - INTERVAL '30 days';
```

**2. Time to Correction** (Target: < 24 hours)
```sql
SELECT
  AVG(corrected_timestamp - classified_timestamp) as avg_time_to_correction
FROM classifications
WHERE corrected_timestamp IS NOT NULL
AND classified_timestamp > NOW() - INTERVAL '30 days';
```

**3. Corrections by Category**
```sql
SELECT
  original_category,
  COUNT(*) as correction_count
FROM classifications
WHERE original_category IS NOT NULL
GROUP BY original_category
ORDER BY correction_count DESC;
```

**4. Repeat Corrections** (same email corrected multiple times)
```sql
SELECT
  email_id,
  COUNT(*) as correction_count
FROM correction_logs
GROUP BY email_id
HAVING COUNT(*) > 3
ORDER BY correction_count DESC;
```

**Target**: Zero repeat corrections (indicates persistent AI issue)

---

## Summary

### Current State (MVP)

‚úÖ **Feedback Mechanism**: Active and working
- Edit classifications directly in Supabase
- Database trigger automatically logs corrections
- No custom UI needed

‚úÖ **Immediate Learning**: Active via vector similarity
- Corrected classifications used as context for future emails
- No retraining required

‚è≥ **Long-Term Learning**: Manual process
- Run monthly analytics queries
- Update AI prompt based on patterns
- Fine-tuning available in Phase 3

### Recommended Workflow

**Daily** (2 minutes):
- Review high-priority classifications in Supabase
- Correct any obvious errors

**Weekly** (15 minutes):
- Review 10-20 recent classifications
- Check correction rate (should be < 5%)
- Correct misclassifications

**Monthly** (30 minutes):
- Run correction analytics queries
- Identify patterns in misclassifications
- Update AI prompt with few-shot examples
- Monitor accuracy improvement

### Next Steps

**Phase 2** (Calendar Integration):
- Continue current feedback process
- Build correction dataset (target: 100+ corrections)

**Phase 3** (Advanced Learning):
- Implement automated prompt updates
- Fine-tune GPT model with correction data
- Build correction UI for easier feedback
- Add confidence-based auto-flagging for review

---

**Current Accuracy**: 100% (no corrections needed yet)
**Correction Logs**: Empty (system performing well)
**Learning Status**: Vector similarity active, prompt tuning manual

üéØ **Feedback system operational and ready for continuous improvement!**
